{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gm7oR_yWsjnk"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlfytRuusjnn"
   },
   "source": [
    "# Lab 4.3: Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xHsJeO0tsjno"
   },
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j-Y7yPDBsjnw"
   },
   "source": [
    "### 1. Forward Feature Selection\n",
    "\n",
    "> Forward Selection: Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "Create a Regression model using Forward Feature Selection by looping over all the features adding one at a time until there are no improvements on the prediction metric ( R2  and  AdjustedR2  in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFSxSn2hsjnz"
   },
   "source": [
    "#### 1.1 Load Diabetics Data Using datasets of sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l2gHGPKVsjn0"
   },
   "outputs": [],
   "source": [
    "## Load the Diabetes Housing dataset\n",
    "\n",
    "# Load the diabetes dataset from sklearn\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 712
    },
    "colab_type": "code",
    "id": "VNyg3soKsjn3",
    "outputId": "230ac33d-8184-4fe3-c144-a594e8ee0a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _diabetes_dataset:\n",
      "\n",
      "Diabetes dataset\n",
      "----------------\n",
      "\n",
      "Ten baseline variables, age, sex, body mass index, average blood\n",
      "pressure, and six blood serum measurements were obtained for each of n =\n",
      "442 diabetes patients, as well as the response of interest, a\n",
      "quantitative measure of disease progression one year after baseline.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "  :Number of Instances: 442\n",
      "\n",
      "  :Number of Attributes: First 10 columns are numeric predictive values\n",
      "\n",
      "  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n",
      "\n",
      "  :Attribute Information:\n",
      "      - age     age in years\n",
      "      - sex\n",
      "      - bmi     body mass index\n",
      "      - bp      average blood pressure\n",
      "      - s1      tc, T-Cells (a type of white blood cells)\n",
      "      - s2      ldl, low-density lipoproteins\n",
      "      - s3      hdl, high-density lipoproteins\n",
      "      - s4      tch, thyroid stimulating hormone\n",
      "      - s5      ltg, lamotrigine\n",
      "      - s6      glu, blood sugar level\n",
      "\n",
      "Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times `n_samples` (i.e. the sum of squares of each column totals 1).\n",
      "\n",
      "Source URL:\n",
      "https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n",
      "\n",
      "For more information see:\n",
      "Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n",
      "(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\n"
     ]
    }
   ],
   "source": [
    "# Description\n",
    "print(diabetes.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Le6yeYXsjn-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>-0.044223</td>\n",
       "      <td>-0.034821</td>\n",
       "      <td>-0.043401</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>-0.017646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.001882</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.051474</td>\n",
       "      <td>-0.026328</td>\n",
       "      <td>-0.008449</td>\n",
       "      <td>-0.019163</td>\n",
       "      <td>0.074412</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.068330</td>\n",
       "      <td>-0.092204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.085299</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.044451</td>\n",
       "      <td>-0.005671</td>\n",
       "      <td>-0.045599</td>\n",
       "      <td>-0.034194</td>\n",
       "      <td>-0.032356</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.089063</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.011595</td>\n",
       "      <td>-0.036656</td>\n",
       "      <td>0.012191</td>\n",
       "      <td>0.024991</td>\n",
       "      <td>-0.036038</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>-0.009362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.005383</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.036385</td>\n",
       "      <td>0.021872</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.015596</td>\n",
       "      <td>0.008142</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>-0.031991</td>\n",
       "      <td>-0.046641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>0.019662</td>\n",
       "      <td>0.059744</td>\n",
       "      <td>-0.005697</td>\n",
       "      <td>-0.002566</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.031193</td>\n",
       "      <td>0.007207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>-0.005515</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>-0.067642</td>\n",
       "      <td>0.049341</td>\n",
       "      <td>0.079165</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.034309</td>\n",
       "      <td>-0.018118</td>\n",
       "      <td>0.044485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>0.041708</td>\n",
       "      <td>0.050680</td>\n",
       "      <td>-0.015906</td>\n",
       "      <td>0.017282</td>\n",
       "      <td>-0.037344</td>\n",
       "      <td>-0.013840</td>\n",
       "      <td>-0.024993</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.046879</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.016318</td>\n",
       "      <td>0.015283</td>\n",
       "      <td>-0.028674</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.044528</td>\n",
       "      <td>-0.025930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>-0.045472</td>\n",
       "      <td>-0.044642</td>\n",
       "      <td>-0.073030</td>\n",
       "      <td>-0.081414</td>\n",
       "      <td>0.083740</td>\n",
       "      <td>0.027809</td>\n",
       "      <td>0.173816</td>\n",
       "      <td>-0.039493</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>0.003064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0    0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401   \n",
       "1   -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412   \n",
       "2    0.085299  0.050680  0.044451 -0.005671 -0.045599 -0.034194 -0.032356   \n",
       "3   -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038   \n",
       "4    0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "437  0.041708  0.050680  0.019662  0.059744 -0.005697 -0.002566 -0.028674   \n",
       "438 -0.005515  0.050680 -0.015906 -0.067642  0.049341  0.079165 -0.028674   \n",
       "439  0.041708  0.050680 -0.015906  0.017282 -0.037344 -0.013840 -0.024993   \n",
       "440 -0.045472 -0.044642  0.039062  0.001215  0.016318  0.015283 -0.028674   \n",
       "441 -0.045472 -0.044642 -0.073030 -0.081414  0.083740  0.027809  0.173816   \n",
       "\n",
       "           s4        s5        s6  \n",
       "0   -0.002592  0.019908 -0.017646  \n",
       "1   -0.039493 -0.068330 -0.092204  \n",
       "2   -0.002592  0.002864 -0.025930  \n",
       "3    0.034309  0.022692 -0.009362  \n",
       "4   -0.002592 -0.031991 -0.046641  \n",
       "..        ...       ...       ...  \n",
       "437 -0.002592  0.031193  0.007207  \n",
       "438  0.034309 -0.018118  0.044485  \n",
       "439 -0.011080 -0.046879  0.015491  \n",
       "440  0.026560  0.044528 -0.025930  \n",
       "441 -0.039493 -0.004220  0.003064  \n",
       "\n",
       "[442 rows x 10 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predictors\n",
    "X = pd.DataFrame(diabetes.data, columns = diabetes.feature_names)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iR8WVIqssjoD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
       "        69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
       "        68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
       "        87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
       "       259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
       "       128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
       "       150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
       "       200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
       "        42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
       "        83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
       "       104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
       "       173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
       "       107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
       "        60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
       "       197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
       "        59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
       "       237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
       "       143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
       "       142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
       "        77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
       "        78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
       "       154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
       "        71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
       "       150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
       "       145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
       "        94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
       "        60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
       "        31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
       "       114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
       "       191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
       "       244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
       "       263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
       "        77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
       "        58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
       "       140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
       "       219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
       "        43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
       "       140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
       "        84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
       "        94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
       "       220.,  57.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target\n",
    "y = diabetes.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QaaJqQxUsjoG"
   },
   "outputs": [],
   "source": [
    "## Create training and testing subsets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9NPcYC45sjoI"
   },
   "source": [
    "#### 1.2 Use Forward Feature Selection to pick a good model\n",
    "\n",
    "**Hint: Same as Lab 4.2.2**\n",
    "\n",
    "- Add R^2 value in a list\n",
    "- Add Adjusted R^2 in another list\n",
    "- Display both R^2 and Adjusted R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xnk9hncbsjoJ"
   },
   "outputs": [],
   "source": [
    "## Flag intermediate output\n",
    "\n",
    "show_steps = False   # for testing/debugging\n",
    "# show_steps = False  # without showing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 7 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-3829748904f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mr2_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\Python\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Python\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'coo'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m         return safe_sparse_dot(X, self.coef_.T,\n\u001b[0m\u001b[0;32m    220\u001b[0m                                dense_output=True) + self.intercept_\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Python\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Python\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 7 is different from 10)"
     ]
    }
   ],
   "source": [
    "r2_list.append(model.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_pred = model.predict(X_train).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_r2_list.append(mean_squared_error(y_train, train_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEqC7hHbsjoN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added feature bmi  with R^2 = 0.366 and adjusted R^2 = 0.358\n",
      "Added feature s5   with R^2 = 0.458 and adjusted R^2 = 0.446\n",
      "Added feature bp   with R^2 = 0.483 and adjusted R^2 = 0.465\n",
      "Added feature s1   with R^2 = 0.499 and adjusted R^2 = 0.475\n",
      "Added feature s2   with R^2 = 0.509 and adjusted R^2 = 0.479\n",
      "Added feature sex  with R^2 = 0.523 and adjusted R^2 = 0.489\n",
      "**************************************************\n",
      "\n",
      "Resulting features:\n",
      "bmi, s5, bp, s1, s2, sex\n"
     ]
    }
   ],
   "source": [
    "## Use Forward Feature Selection to pick a good model\n",
    "\n",
    "# start with no predictors\n",
    "included = []\n",
    "# keep track of model and parameters\n",
    "best = {'feature': '', 'r2': 0, 'a_r2': 0}\n",
    "# create a model object to hold the modelling parameters\n",
    "model = LinearRegression()\n",
    "# get the number of cases in the test data\n",
    "n = X_test.shape[0]\n",
    "\n",
    "r2_list = []\n",
    "adjusted_r2_list = []\n",
    "\n",
    "while True:\n",
    "    changed = False\n",
    "    \n",
    "    if show_steps:\n",
    "        print('') \n",
    "\n",
    "    # list the features to be evaluated\n",
    "    excluded = list(set(X.columns) - set(included))\n",
    "    \n",
    "    if show_steps:\n",
    "        print('(Step) Excluded = %s' % ', '.join(excluded))  \n",
    "\n",
    "    # for each remaining feature to be evaluated\n",
    "    for new_column in excluded:\n",
    "        \n",
    "        if show_steps:\n",
    "            print('(Step) Trying %s...' % new_column)\n",
    "            print('(Step) - Features = %s' % ', '.join(included + [new_column]))\n",
    "\n",
    "        # fit the model with the Training data\n",
    "        fit = model.fit(X_train[included + [new_column]], y_train)\n",
    "        # calculate the score (R^2 for Regression)\n",
    "        r2 = fit.score(X_train[included + [new_column]], y_train)\n",
    "        # number of predictors in this model\n",
    "        k = len(included + [new_column])\n",
    "        # calculate the adjusted R^2\n",
    "        adjusted_r2 = 1 - ( ( (1 - r2) * (n - 1) ) / (n - k - 1) )\n",
    "\n",
    "        if show_steps:\n",
    "            print('(Step) - Adjusted R^2: This = %.3f; Best = %.3f' % \n",
    "                  (adjusted_r2, best['a_r2']))\n",
    "\n",
    "        # if model improves\n",
    "        if adjusted_r2 > best['a_r2']:\n",
    "            # record new parameters\n",
    "            best = {'feature': new_column, 'r2': r2, 'a_r2': adjusted_r2}\n",
    "            # flag that found a better model\n",
    "            changed = True\n",
    "            if show_steps:\n",
    "                print('(Step) - New Best!   : Feature = %s; R^2 = %.3f; Adjusted R^2 = %.3f' % \n",
    "                      (best['feature'], best['r2'], best['a_r2']))\n",
    "    # END for\n",
    "\n",
    "    # if found a better model after testing all remaining features\n",
    "    if changed:\n",
    "        # update control details\n",
    "        included.append(best['feature'])\n",
    "        excluded = list(set(excluded) - set(best['feature']))\n",
    "        print('Added feature %-4s with R^2 = %.3f and adjusted R^2 = %.3f' % \n",
    "              (best['feature'], best['r2'], best['a_r2']))\n",
    "    else:\n",
    "        # terminate if no better model\n",
    "        print('*'*50)\n",
    "        break\n",
    "\n",
    "print('')\n",
    "print('Resulting features:')\n",
    "print(', '.join(included))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "colab_type": "code",
    "id": "whvMHo6rsjoU",
    "outputId": "0f05a34f-f5bd-46ac-9798-73b43c8e4d95"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Chart both R^2 and Adjusted R^2\n",
    "\n",
    "_range = range(1, len(r2_list)+1)\n",
    "\n",
    "# define chart size\n",
    "plt.figure(figsize = (10, 5))\n",
    "# plot each metric \n",
    "plt.plot(_range, r2_list, label = '$R^2$')\n",
    "plt.plot(_range, adjusted_r2_list, label = '$Adjusted \\: R^2$')\n",
    "# add some better visualisation\n",
    "plt.xlabel('Number of Features')\n",
    "plt.legend()\n",
    "# output the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZNsyA27Dlwxs"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > © 2021 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "IOD_Lab 4.3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
